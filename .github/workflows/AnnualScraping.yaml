name: Annual Scrape and Upload

on:
  schedule:
    # Runs at 00:00 UTC on August 23rd each year
    - cron: '0 0 23 8 *'

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11.3'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install scrapy supabase python-dotenv

    - name: Run Scrapy spiders
      run: python runAll.py

    - name: Commit CSV files
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add *.csv
        git commit -m "Commit CSV files annually" 
        git push

    - name: Upload to Supabase
  env:
    SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
    SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
  run: |
    python -c "
import os
import csv
from supabase import create_client
from dotenv import load_dotenv
import glob

# Load environment variables
load_dotenv()

url = os.environ.get('SUPABASE_URL')
key = os.environ.get('SUPABASE_KEY')
supabase = create_client(url, key)

for csv_file in glob.glob('*.csv'):
    table_name = os.path.splitext(csv_file)[0]
    print(f'Processing {csv_file} into table {table_name}')
    
    with open(csv_file, mode='r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        
        if rows:
            response = supabase.table(table_name).insert(rows).execute()
            print(f'Uploaded {len(rows)} rows to {table_name}')
        else:
            print(f'No data found in {csv_file}')
    "
